
@article{seo_fast_2016,
	title = {Fast {Contour}-{Tracing} {Algorithm} {Based} on a {Pixel}-{Following} {Method} for {Image} {Sensors}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/16/3/353},
	doi = {10.3390/s16030353},
	abstract = {Contour pixels distinguish objects from the background. Tracing and extracting contour pixels are widely used for smart/wearable image sensor devices, because these are simple and useful for detecting objects. In this paper, we present a novel contour-tracing algorithm for fast and accurate contour following. The proposed algorithm classifies the type of contour pixel, based on its local pattern. Then, it traces the next contour using the previous pixel’s type. Therefore, it can classify the type of contour pixels as a straight line, inner corner, outer corner and inner-outer corner, and it can extract pixels of a specific contour type. Moreover, it can trace contour pixels rapidly because it can determine the local minimal path using the contour case. In addition, the proposed algorithm is capable of the compressing data of contour pixels using the representative points and inner-outer corner points, and it can accurately restore the contour image from the data. To compare the performance of the proposed algorithm to that of conventional techniques, we measure their processing time and accuracy. In the experimental results, the proposed algorithm shows better performance compared to the others. Furthermore, it can provide the compressed data of contour pixels and restore them accurately, including the inner-outer corner, which cannot be restored using conventional algorithms.},
	language = {en},
	number = {3},
	urldate = {2020-09-11},
	journal = {Sensors},
	author = {Seo, Jonghoon and Chae, Seungho and Shim, Jinwook and Kim, Dongchul and Cheong, Cheolho and Han, Tack-Don},
	month = mar,
	year = {2016},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {boundary following, contour data compression, contour tracing, pixel following},
	pages = {353},
	file = {Full Text PDF:/Users/jonghoon.seo/Zotero/storage/2TDNV8AD/Seo 등 - 2016 - Fast Contour-Tracing Algorithm Based on a Pixel-Fo.pdf:application/pdf;Snapshot:/Users/jonghoon.seo/Zotero/storage/Z8WL6KGN/353.html:text/html}
}

@article{shim_gesture-based_2016,
	title = {Gesture-based interactive augmented reality content authoring system using {HMD}},
	volume = {20},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-016-0282-z},
	doi = {10.1007/s10055-016-0282-z},
	abstract = {This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a marker via gestures. The purpose of this system is to simplify augmented reality (AR) technology usage for ordinary users, especially parents and preschool children who are unfamiliar with AR technology. The system provides an immersive AR environment with a head-mounted display and recognizes users’ gestures via an RGB-D camera. Users can freely create the AR content that they will be using without any special programming ability simply by connecting virtual objects stored in a database to the system. Following recognition of the marker via the system’s RGB-D camera worn by the user, he/she can apply various interactive features to the marker-based AR content using simple gestures. Interactive features applied to AR content can enlarge, shrink, rotate, and transfer virtual objects with hand gestures. In addition to this gesture-interactive feature, the proposed system also allows for tangible interaction using markers. The AR content that the user edits is stored in a database, and is retrieved whenever the markers are recognized. The results of comparative experiments conducted indicate that the proposed system is easier to use and has a higher interaction satisfaction level than AR environments such as fixed-monitor and touch-based interaction on mobile screens.},
	language = {en},
	number = {1},
	urldate = {2020-09-11},
	journal = {Virtual Reality},
	author = {Shim, Jinwook and Yang, Yoonsik and Kang, Nahyung and Seo, Jonghoon and Han, Tack-Don},
	month = mar,
	year = {2016},
	pages = {57--69}
}

@article{__2013,
	title = {피부색 및 깊이정보를 이용한 영역채움 기반 손 분리 기법},
	volume = {16},
	issn = {1229-7771},
	url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE02277885},
	abstract = {영상에서 배경을 제거하고 손을 분리하는 기술은 손 인식 연구에서 가장 먼저 수행되는 기술이며, 분리된 결과 영상의 성능에 따라 이후의 인식 단계의 성능이 결정되는 중요한 기술이다. 기존의 연구는 조명 및 배경의 변화에 취약하거나 다수의 사용자와 상호작용에 한계가 있었다. 본 논문에서는 컬러 영상과 깊이 영상용 혼용하여 손을 분리하는 기술을 제안한다. 먼저 입력된 컬러 영상을 이용하여 복잡한 환경에서도 정확하게 영역 채용을 위한 초기 위치를 설정하였다. 이 위치를 기준으로 영역 채용 연산을 위한 한계 영역을 재설정하여 조명 변화로 침식된 영역을 포함하도록 하고 깊이 영상에서 영역 채용 연산을 수행함으로써 조명과 환경의 변화에도 강인하게 손의 영역용 분리하도록 하였다. 또한 이렇게 분리된 손의 영역을 이용하여 실시간으로 피부 모델을 학습함으로써 조명 환경에 적응적으로 피부 모델을 갱신하여 보다 강인한 인식 성능을 얻을 수 있었다. 이를 다양한 조명 및 배경 환경에서 기존의 알고리즘과 비교 실험을 수행하여 강인한 인식 성능을 확인할 수 있었으며 특히 역광 환경과 같이 조명 변화기 극심한 환경에서 강인한 성능을 보여주었다.},
	language = {ko},
	number = {9},
	urldate = {2020-09-11},
	journal = {멀티미디어학회논문지},
	author = {서종훈 and 채승호 and 심진욱 and 김하영 and 한탁돈},
	month = sep,
	year = {2013},
	note = {ISSN: 1229-7771
Issue: 9
Pages: 1031-1043
Volume: 16},
	pages = {1031--1043},
	file = {Snapshot:/Users/jonghoon.seo/Zotero/storage/UMHJDVJT/articleDetail.html:text/html}
}

@article{seo_region-growing_nodate,
	title = {Region-growing based {Hand} {Segmentation} {Algorithm} using {Skin} {Color} and {Depth} {Information}},
	volume = {16},
	issn = {1229-7771},
	url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE02277885},
	number = {9},
	urldate = {2020-09-11},
	journal = {Journal of Korea Multimedia Society},
	author = {Seo, Jonghoon and Chae, Seungho and Shim, Jinwook and Hayoung, Kim and Tack-Don, Han},
	pages = {1031--1043}
}

@article{__2013-1,
	title = {마커 및 제스처 상호작용이 가능한 증강현실 저작도구},
	volume = {16},
	issn = {1229-7771},
	url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE02224153},
	abstract = {본 논문에서는 사용자들이 손쉽게 마커기반과 제스처 상호작용 방법들을 적용한 증강현실 콘텐츠를 제작할 수 있는 증강현실 저작도구 시스템을 제안한다. 기존의 증강현실 저작도구들은 가상의 객체를 증강하는데 초점이 맞춰져 있었고, 이러한 증강현실 콘텐츠와 상호작용을 하기 위해서는 사용자가 마커나 센서를 이용하는 방법을 사용하였다. 우리는 이러한 제한적인 상호작용 방법의 문제점을 마커기반 상호작용 방법과 깊이 인식 카메라인 Kinect를 사용한 제스처 상호작용 방법을 적용시킴으로써 해결하고자 한다. 제안하는 시스템에서는 사용자가 인터페이스를 통하여 간단한 형태의 마커기반 증강현실 콘텐츠를 쉽게 제작할 수 있다. 또한, 능동적으로 사용자가 증강현실 콘텐츠와 상호작용할 수 있는 방법들을 제공하고 있다. 본 연구에서 제공하는 상호작용 방법으로는 마커기반의 상호작용 방법으로 2개의 마커를 이용한 방법과 마커의 가림현상(Occlusion)을 이용한 방법이 있다. 그리고 사용자의 맨손을 인식, 추적하여 객체의 확대 축소, 이동, 회전이 가능한 제스처 상호작용 방법을 제공한다. 저작도구 시스템에 대한 사용성 평가와 마커 및 제스처 상호작용에 대한 사용성을 비교평가하여, 본 연구의 긍정적 결과를 확인하였다.},
	language = {ko},
	number = {6},
	urldate = {2020-09-11},
	journal = {멀티미디어학회논문지},
	author = {심진욱 and 공민제 and 김하영 and 채승호 and 정경호 and 서종훈 and 한탁돈},
	month = jun,
	year = {2013},
	note = {ISSN: 1229-7771
Issue: 6
Pages: 720-734
Volume: 16},
	pages = {720--734},
	file = {Snapshot:/Users/jonghoon.seo/Zotero/storage/7FF4WLX6/articleDetail.html:text/html}
}

@article{__nodate,
	title = {차세대 디지털교과서를 위한 기반기술 및 적용에 관한 연구},
	volume = {14},
	issn = {1229-3245},
	url = {https://www.earticle.net/Article/A121727},
	abstract = {현재 진행되고 있는 디지털교과서 시범사업은 연구학교를 중심으로 다양한 컨텐츠 활용 및 연구사례 등을 도출하고 있다. 그러나 현재의 디지털교과서는 기존 온라인 학습콘텐츠와 유사한 상호작용 및 교수학습 모델을 제공하며 특히 태블릿 기반 콘텐츠의 학습효과 및 탐구능력 등과 같은 연구사례가 부족한 실정이다.본 논문에서는 태블릿 기반의 차세대 디지털교과서를 위한 교수학습 모델을 제안하며 특히 디지털 잉크 기반의 스케치 인터페이스 및 증강현실 기술을 이용한 상호작용 모델을 설계하였다. 제안된 연구기법은 학습콘텐츠로 구현되었으며 이를 통한 교수자와 학습자간의 활발한 상호작용 및 학습몰입감의 효과를 규명하였다. 본 연구의 결과는 향후 미래형 디지털교과서 개발 및 적용 시 효과적인 개발방안이 될 것이다.},
	language = {ko},
	number = {2},
	urldate = {2020-09-11},
	journal = {정보교육학회논문지},
	author = {손원성 and 한재협 and 최진용 and 서종훈 and 최윤철 and 한탁돈 and 임순범},
	pages = {165--174},
	file = {Snapshot:/Users/jonghoon.seo/Zotero/storage/YR6LE93M/A121727.html:text/html}
}

@article{shim_augmented_2013,
	title = {Augmented {Reality} {Authoring} {Tool} with {Marker} \& {Gesture} {Interactive} {Features}},
	volume = {16},
	issn = {1229-7771},
	url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE02224153},
	number = {6},
	urldate = {2020-09-11},
	journal = {Journal of Korea Multimedia Society},
	author = {Shim, Jinwook and Kong, Minje and Kim, Hayoung and Chae, Seungho and Jung, Kyungho and Seo, Jonghoon and Han, Tack-Don},
	month = jun,
	year = {2013},
	pages = {720--734}
}

@article{sohn_fundamental_nodate,
	title = {Fundamental study on the technology and application for the {Next} {Generation} {Digital} {Textbook}},
	volume = {14},
	issn = {1229-3245},
	url = {https://www.earticle.net/Article/A121727},
	number = {2},
	journal = {Journal of The Korean Association of Information Education},
	author = {Sohn, Won-Sung and Han, Jae-Hyeop and Choy, Jin-Yong and Seo, Jonghoon and Choy, Yoon-Chul and Han, Tack-Don and Lim, Soon-Bum},
	pages = {165--174}
}

@inproceedings{kim_realtime_2017,
	title = {Realtime plane detection for projection {Augmented} {Reality} in an unknown environment},
	doi = {10.1109/ICASSP.2017.7953305},
	abstract = {We propose a realtime plane detection method for projection-based Augmented Reality (AR) system in an unknown environment. While previous works usually designate space, the plane detection method automatically detects multiple planes based on the proposed constrained sampling strategy in RAndom SAmpling Concensus (RANSAC). For each plane, an area for projection is selected for contents while considering occlusions by other objects. In addition, when the multiple planes are detected, the importance for contents is measured by the score functions based on the properties of planes such as size, color, and position. The proposed method can guide users to select plane for projection by visualizing the importances, or can automatically select a plane according to the users. We achieves a significant improvement in speed (about 260 times faster than the RANSAC) and high precision. These technique has become widely utilized in various AR applications such as AR game, and etc.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Kim, Dongchul and Chae, Seungho and Seo, Jonghoon and Yang, Yoonsik and Han, Tack-Don},
	month = mar,
	year = {2017},
	note = {ISSN: 2379-190X},
	keywords = {AR applications, AR game, augmented reality, Augmented reality, Cameras, constrained sampling strategy, data visualisation, Distortion, Image color analysis, Internet of Things (IoT), optical projectors, Plane detection, Projection AR, projection augmented reality, Projector-camera System, random processes, random sampling concensus, RANSAC, realtime plane detection, sampling methods, score functions, Sensor systems, Signal processing for Smart space, Three-dimensional displays, unknown environment},
	pages = {5985--5989},
	file = {IEEE Xplore Abstract Record:/Users/jonghoon.seo/Zotero/storage/438L5E5Y/7953305.html:text/html}
}

@inproceedings{chae_colorcodear_2016,
	title = {{ColorCodeAR}: {Large} identifiable {ColorCode}-based augmented reality system},
	shorttitle = {{ColorCodeAR}},
	doi = {10.1109/SMC.2016.7844630},
	abstract = {Augmented reality (AR) is widely used in various applications of computer vision, such as marker-based AR and markerless-based AR. These AR techniques are used in various fields, including industry, education, and medicine. Using marker-based AR, employees can easily perform step-by-step maintenance and repairs, and they can register parts information for large plants. However, conventional marker-based AR relies on a relatively small number of recognizable IDs compared to barcode markers. In this paper, to address the insufficient identification volume in conventional AR systems, we integrate barcode-based code technology with marker-based AR technology. Based on the results of an experiment, we applied ColorCode to our marker-based AR system. Nevertheless, difficulties arise when applying ColorCode to an AR system, owing to its recognition distance and relatively small size, compared to other AR codes. In this paper, therefore, we complemented quad detection with a tracking technique for various angles and distances, facilitating reliable recognition of the color-code-based AR system, Moreover, we added a tracking module to address the system's failure to detect markers. The experimental results demonstrate that the proposed system offers stable recognition.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Chae, Seungho and Seo, Jonghoon and Yang, Yoonsik and Han, Tack-Don},
	month = oct,
	year = {2016},
	keywords = {augmented reality, bar codes, barcode-based code technology, ColorCodeAR, computer vision, Conferences, Cybernetics, Distortion, identifiable ColorCode-based augmented reality system, Image color analysis, image colour analysis, Image recognition, Maintenance engineering, marker-based AR technology, object detection, object tracking, quaddetection, tracking technique, Two dimensional displays},
	pages = {002598--002602},
	file = {IEEE Xplore Abstract Record:/Users/jonghoon.seo/Zotero/storage/MXGMU32X/citations.html:text/html}
}

@inproceedings{chae_lasertouch_2014,
	address = {New York, NY, USA},
	series = {{SA} '14},
	title = {{LaserTouch}: touch device control using a laser pointer},
	isbn = {978-1-4503-2792-3},
	shorttitle = {{LaserTouch}},
	url = {https://doi.org/10.1145/2668975.2668982},
	doi = {10.1145/2668975.2668982},
	abstract = {The touch device has widely been used because of its advantage of intuitive control, and a variety of applications that utilize them are currently being developed. In addition, it is being used in various fields as devices in the form of smartphone and pad are released, and the increasing user needs for touch devices raised the need to conduct studies on the remote control of touch devices. However, since touch devices do not have a separate controller, we can control the device only by user's direct touch. So it is inconvenient when watching videos with a device on the holder, or controlling contents on a large table-top environment.},
	urldate = {2020-09-11},
	booktitle = {{SIGGRAPH} {Asia} 2014 {Posters}},
	publisher = {Association for Computing Machinery},
	author = {Chae, Seungho and Seo, Jonghoon and Yang, Yoonsik and Han, Tack-Don},
	month = nov,
	year = {2014},
	pages = {1}
}

@inproceedings{kim_ar_2014,
	address = {New York, NY, USA},
	series = {{IUI} '14},
	title = {{AR} {Lamp}: interactions on projection-based augmented reality for interactive learning},
	isbn = {978-1-4503-2184-6},
	shorttitle = {{AR} {Lamp}},
	url = {https://doi.org/10.1145/2557500.2557505},
	doi = {10.1145/2557500.2557505},
	abstract = {Today, people use a computer almost everywhere. At the same time, they still do their work in the old-fashioned way, such as using a pen and paper. A pen is often used in many fields because it is easy to use and familiar. On the other hand, however, it is a quite inconvenient because the information printed on paper is static. If digital features are added to this paper environment, the users can do their work more easily and efficiently. AR (augmented reality) Lamp is a stand-type projector and camera embedded system with the form factor of a desk lamp. Its users can modify the virtually augmented content on top of the paper with seamlessly combined virtual and physical worlds. AR is quite appealing, but it is difficult to popularize due to the lack of interaction. In this paper, the interaction methods that people can use easily and intuitively are focused on. A high-fidelity prototype of the system is presented, and a set of novel interactions is demonstrated. A pilot evaluation of the system is also reported to explore its usage possibility.},
	urldate = {2020-09-11},
	booktitle = {Proceedings of the 19th international conference on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Jeongyun and Seo, Jonghoon and Han, Tack-Don},
	month = feb,
	year = {2014},
	keywords = {bimanual interaction, finger gesture, pen computing, projection-based augmented reality},
	pages = {353--358}
}

@inproceedings{choi_real-time_2014,
	title = {Real-time {Tour} {Map} {Recognition} and {Tracking} for {Mobile} {Tour} {Guide} {Systems}},
	url = {http://www.imrc.kist.re.kr/wiki/KJMR2014},
	booktitle = {7th {Korea}-{Japan} {Workshop} on {Mixed} {Reality}({KJMR}) 2014},
	author = {Choi, Heeseung and Seo, Jonghoon and Chae, Seungho and Yang, Yoonsik and Kim, Ig-Jae},
	month = apr,
	year = {2014}
}

@inproceedings{shim_interactive_2014,
	title = {Interactive features based augmented reality authoring tool},
	doi = {10.1109/ICCE.2014.6775902},
	abstract = {This paper intends to propose an authoring tool system that allows users to easily create augmented reality content with the application of marker based and gesture interactions. It is possible to generate a simplified form of marker based augmented reality content with the user interface by using Kinect that enables gesture interaction. We seek to provide methods for the user to actively interact with augmented reality content. The interaction method we put forth in this study is a gesture interaction approach capable of Transfer, Rotate, Enlarge, and Shrink an object by recognizing and tracking the user's bare hand. As for marker based interaction methods, there is one employing two markers and another using marker occlusion. We ascertain the positive results of this study from the user evaluation of an authoring tool system and marker based and gesture interactions.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Shim, Jinwook and Kong, Minje and Yang, Yoonsik and Seo, Jonghoon and Han, Tack-Don},
	month = jan,
	year = {2014},
	note = {ISSN: 2158-4001},
	keywords = {augmented reality, Augmented reality, authoring systems, Computer vision, enlarge, gesture interactions, gesture recognition, Image segmentation, interactive devices, interactive features based augmented reality authoring tool, Kinect, marker based augmented reality content, marker based interaction, marker occlusion, object recognition, object tracking, Real-time systems, rotate, shrink, Thumb, transfer, user bare hand recognition, user bare hand tracking, user interface, User interfaces},
	pages = {47--50},
	file = {IEEE Xplore Abstract Record:/Users/jonghoon.seo/Zotero/storage/P8L43VZH/6775902.html:text/html}
}

@inproceedings{seo_imaged_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Imaged {Based} {Codes} {Performance} {Comparison} for {Mobile} {Environments}},
	isbn = {978-3-642-21602-2},
	doi = {10.1007/978-3-642-21602-2_71},
	abstract = {By spreading of smart phones, mobile barcodes are used widely. However, there are so many 2D barcodes to be available. So, it is important to compare those mobile barcodes. In this paper we performed decoding performance comparison between popular mobile barcodes. ColorCode is using color information to get information. So it shows most improved performance in distance and size. Also, it can provide magnifying decoding mode, and it shows more enhancing result.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Design} and {Development} {Approaches}},
	publisher = {Springer},
	author = {Seo, Jonghoon and Choi, Ji Hye and Han, Tack-don},
	editor = {Jacko, Julie A.},
	year = {2011},
	keywords = {2D Barcode, Image based Code},
	pages = {653--659},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/YJEVWC96/Seo 등 - 2011 - Imaged Based Codes Performance Comparison for Mobi.pdf:application/pdf}
}

@inproceedings{seo_enhancing_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Enhancing {Marker}-{Based} {AR} {Technology}},
	isbn = {978-3-642-22021-0},
	doi = {10.1007/978-3-642-22021-0_12},
	abstract = {In this paper, we propose a method that solves both jittering and occlusion problems which is the biggest issue in marker based augmented reality technology. Because we adjust the pose estimation by using multiple keypoints that exist in the marker based on cells, we can predict the strong pose on jittering. Additionally, we can solve the occlusion problem by applying tracking technology.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {New} {Trends}},
	publisher = {Springer},
	author = {Seo, Jonghoon and Shim, Jinwook and Choi, Ji Hye and Park, James and Han, Tack-don},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {Augmented Reality, Marker-based AR, Tracking},
	pages = {97--104},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/2D2IS78G/Seo 등 - 2011 - Enhancing Marker-Based AR Technology.pdf:application/pdf}
}

@inproceedings{park_colorit_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ColoriT}: {Color} {Based} {Image} {Code} {Application} to {Aid} in {Memory} {Restoration} of {Offline} {Photo} {Artifacts}},
	isbn = {978-3-642-21602-2},
	shorttitle = {{ColoriT}},
	doi = {10.1007/978-3-642-21602-2_69},
	abstract = {Many areas of research have applied memory aid applications to help users remember experiences or enhance learning abilities. Relatively little study however has been done on the use of color based image code as a memory aid. In fact much of what is in use by mainstream media and businesses use mobile barcodes such as 2D image code like Quick Response (QR) code and Microsoft Tag for accessing online content. Part of the freedom of using mobile code is many formats are freely based image code and have accessible API and development kits. Only few are licensed based and are limited to developing usable applications. We investigate a proprietary licensed color based image code using an application we developed called ColoriT (pronounced Color- ət) in hopes of studying its applicability to enhance memory aid when viewing photo artifacts. ColoriT is a simple photo memory tagging concept used to tag offline photos in a pervasive and natural way. By enhancing the user’s ability to tag offline photos with memory artifacts we create a tool inspired by storytelling to improve memory aid and overall enjoyment of looking at photos.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Design} and {Development} {Approaches}},
	publisher = {Springer},
	author = {Park, James and Seo, Jonghoon and Choi, Ji-Hye and Han, Tackdon},
	editor = {Jacko, Julie A.},
	year = {2011},
	keywords = {2D barcode, Color, Colorcode, Memory aid, Sound artifacts},
	pages = {637--642},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/WAQUX4MD/Park 등 - 2011 - ColoriT Color Based Image Code Application to Aid.pdf:application/pdf}
}

@inproceedings{shim_msl_ar_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MSL}\_AR {Toolkit}: {AR} {Authoring} {Tool} with {Interactive} {Features}},
	isbn = {978-3-642-22021-0},
	shorttitle = {{MSL}\_AR {Toolkit}},
	doi = {10.1007/978-3-642-22021-0_13},
	abstract = {We describe an authoring tool for Augmented Reality (AR) contents. In recent years there have been a number of frameworks proposed for developing Augmented Reality (AR) applications. This paper describes an authoring tool for Augmented Reality (AR) application with interactive features. We developed the AR authoring tool which provides Interactive features that we can perform the education service project and participate it actively for the participating education service. In this paper, we describe MSL\_AR Authoring tool process and two kinds of interactive features.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {New} {Trends}},
	publisher = {Springer},
	author = {Shim, Jinwook and Seo, Jonghoon and Han, Tack-don},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {Augmented Reality, Authoring, interaction},
	pages = {105--112},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/SK63W9YP/Shim 등 - 2011 - MSL_AR Toolkit AR Authoring Tool with Interactive.pdf:application/pdf}
}

@inproceedings{baek_meet_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Meet {Researcher} in the {Real} {World} {Using} the {ConAR}: {Context}-{Aware} {Researcher}},
	isbn = {978-3-642-21796-8},
	shorttitle = {Meet {Researcher} in the {Real} {World} {Using} the {ConAR}},
	doi = {10.1007/978-3-642-21796-8_2},
	abstract = {In this paper, we focused on a research on a Location-Based Social Network Service to increase social interactions among people who are engaging in research and development. Since existing many of location-based social network services have only provided information of regions and social relations of users have targeted an unspecific majority of groups, they could not have active interactions, compared to users of online communities who have particular use purposes. ConAR what we designed to get over these weaknesses, aims at researches to academically exchange with others and it enables them to have social interactions with people of the same interests by helping them select discussion topics and decide assignment places simply and on impulse.},
	language = {en},
	booktitle = {Online {Communities} and {Social} {Computing}},
	publisher = {Springer},
	author = {Baek, Sung-Wook and Seo, Jong-Hoon and Han, Tack-Don},
	editor = {Ozok, A. Ant and Zaphiris, Panayiotis},
	year = {2011},
	keywords = {Communities, Context-Aware Service, Human Computer Interaction, Location-Based Service, Social Computing, Social Interaction},
	pages = {12--19},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/E76ZMPK4/Baek 등 - 2011 - Meet Researcher in the Real World Using the ConAR.pdf:application/pdf}
}

@inproceedings{yoon_usability_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Usability} {Evaluation} of {Public} {Icon} {Interface}},
	isbn = {978-3-642-02574-7},
	doi = {10.1007/978-3-642-02574-7_61},
	abstract = {Existing image codes interface needs additional visual marker and explanation of the service. To overcome these limitations, there were some researches to use a public icon as an anchor. The public icon is human-readable and does not need additional visual marker or explanation. In this paper, we carried out the usability evaluation of the public icon interface with a high-fidelity prototype in comparison to the existing image code. In addition, we analyze user preferences from the results. From the analysis, we perceived that the public icon interface is better to use in the public because the public icon interface is familiar with people and doesn’t need additional materials or much cognitive load and are in good harmony with current environments.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {New} {Trends}},
	publisher = {Springer},
	author = {Yoon, Sungyoung and Seo, Jonghoon and Yoon, Joonyoung and Shin, Seungchul and Han, Tack-Don},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {barcode, color-based image code, image code, pictogram, Public icon},
	pages = {540--546},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/9UJMYSR9/Yoon 등 - 2009 - A Usability Evaluation of Public Icon Interface.pdf:application/pdf}
}

@inproceedings{kim_ar_2013,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '13},
	title = {{AR} pen and hand gestures: a new tool for pen drawings},
	isbn = {978-1-4503-1952-2},
	shorttitle = {{AR} pen and hand gestures},
	url = {https://doi.org/10.1145/2468356.2468525},
	doi = {10.1145/2468356.2468525},
	abstract = {This paper explores the interaction possibilities when artists use their non-dominant hand, while drawing with a pen in their dominant hand. We propose a new interactive AR-based pen tool which can overlay virtual images onto a physical drawing in real time. This system allows artists to control the augmented images with gestures of a non-dominant hand while drawing. By interacting with the visually augmented contents using hand gestures and a pen bimanually, artists can draw pictures more creatively. We also made a standalone pen system integrated with a pico-projector and a camera, and suggest a set of useful scenarios for the conventional pen-and-paper drawing.},
	urldate = {2020-09-11},
	booktitle = {{CHI} '13 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Hark-Joon and Kim, Hayoung and Chae, Seungho and Seo, Jonghoon and Han, Tack-Don},
	month = apr,
	year = {2013},
	keywords = {bimanual interaction, hand gesture, pen-based computing, projection-based augmented reality},
	pages = {943--948}
}

@inproceedings{lee_real-time_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Real}-{Time} {Hand} {Interaction} {System} for {Image} {Sensor} {Based} {Interface}},
	isbn = {978-3-642-02577-8},
	doi = {10.1007/978-3-642-02577-8_23},
	abstract = {Diverse sensors are available in ubiquitous computing of which resources is inherent in environment. Among them, image sensor acquires necessary data using camera without any extra devices, which is a different aspect from other sensors. It can provide additional services and/or applications by using a location of code and ID in real time image. Focusing on this, Intuitive interface operating method in ubiquitous computing environment that has plenty of image codes is suggested. GUI using image sensor was designed, which works real-time interactive operation between user and the GUI without any additional button or device. This interface method recognizes user’s hand images in real-time by learning them at a starting point. The method sets interaction point, and operates the GUI through hand gestures defined previously. We expect this study can be adopted to augmented reality area and real time interface using user’s hand.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Novel} {Interaction} {Methods} and {Techniques}},
	publisher = {Springer},
	author = {Lee, SeIn and Seo, Jonghoon and Lim, Soon-bum and Choy, Yoon-Chul and Han, TackDon},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {Augmented Reality, Hand interaction, Hand interface, HCI},
	pages = {208--215},
	file = {Springer Full Text PDF:/Users/jonghoon.seo/Zotero/storage/6Y5FI8QA/Lee 등 - 2009 - A Real-Time Hand Interaction System for Image Sens.pdf:application/pdf}
}

@inproceedings{kim_tag_2006,
	title = {Tag interface for pervasive computing: {Paper} tag interface using imae code},
	shorttitle = {Tag interface for pervasive computing},
	url = {https://yonsei.pure.elsevier.com/en/publications/tag-interface-for-pervasive-computing-paper-tag-interface-using-i},
	language = {English},
	urldate = {2020-09-11},
	booktitle = {{SIGMAP} 2006 - {International} {Conference} on {Signal} {Processing} and {Multimedia} {Applications}, {Proceedings}},
	author = {Kim, Dong Chul and Seo, Jong Hoon and Cheong, Cheolho and Han, Tack Don},
	month = dec,
	year = {2006},
	pages = {356--359},
	file = {Snapshot:/Users/jonghoon.seo/Zotero/storage/9ZNLZTER/tag-interface-for-pervasive-computing-paper-tag-interface-using-i.html:text/html}
}

@patent{kim_mobile_2017,
	title = {Mobile terminal and method for controlling the same},
	url = {https://patents.google.com/patent/US20170150227/en},
	nationality = {US},
	language = {en},
	assignee = {LG Electronics Inc},
	number = {US20170150227A1},
	urldate = {2020-09-11},
	author = {KIM, Suhwang and SEO, Jonghoon and HONG, Soonwon and YANG, Hongbin and JANG, Jeonghun},
	month = may,
	year = {2017},
	keywords = {device, display, electronic device, external device, payment},
	file = {Fulltext PDF:/Users/jonghoon.seo/Zotero/storage/RFT7BIQW/KIM 등 - 2017 - Mobile terminal and method for controlling the sam.pdf:application/pdf}
}

@patent{kim_electronic_2020,
	title = {Electronic device and control method thereof},
	url = {https://patents.google.com/patent/CN107071540B/en},
	abstract = {An electronic apparatus and a control method thereof. An electronic device and a method for controlling the same are disclosed herein. In an aspect, when a mirror connection request is received from a first external device, a second external device that is pair-connected with the first external device is also mirrored. Accordingly, the mirror image of the first external device and the mirror image of the second external device may be displayed together.},
	nationality = {CN},
	language = {en},
	assignee = {Lg Electronics Inc},
	number = {CN107071540B},
	urldate = {2020-09-11},
	author = {Kim, Suhwang and Seo, Jonghoon and Hong, Soonwon and Yang, Hongbin and Jang, Jeonghun},
	month = mar,
	year = {2020},
	keywords = {device, display, electronic device, external device, payment},
	file = {Fulltext PDF:/Users/jonghoon.seo/Zotero/storage/MUCAX2IL/金秀晃 등 - 2020 - Electronic device and control method thereof.pdf:application/pdf}
}

@patent{__2017,
	title = {전자 기기 및 전자 기기의 제어 방법},
	url = {https://doi.org/10.8080/1020150162864},
	abstract = {The present invention provides an electronic device, and a method for controlling the electronic device. The electronic device can perform a product payment function by using a payment application executed in an external device through mirroring connection with the external device when the payment function of a product connected to broadcasting content is activated through a remote signal of a wireless input device.},
	nationality = {KR},
	assignee = {엘지전자 주식회사},
	number = {10-2017-0058795},
	urldate = {2020-09-11},
	author = {김수황 and 서종훈 and 홍순원 and 양홍빈 and 장정훈},
	month = may,
	year = {2017},
	keywords = {display unit, electronic device, external device, mirroring, payment},
	file = {Fulltext PDF:/Users/jonghoon.seo/Zotero/storage/B8M6XZ5Y/김수황 등 - 2017 - Electronic device and method for controlling the s.pdf:application/pdf}
}

@patent{__2016,
	title = {바코드 인식 장치 및 방법},
	url = {http://goo.gl/blEyD8},
	nationality = {KR},
	language = {ko},
	number = {10-1632380},
	urldate = {2020-09-11},
	author = {서종훈 and 한탁돈 and 김동철 and 신해준 and 윤형민 and 최창규},
	month = jun,
	year = {2016},
	file = {바코드 인식 장치 및 방법 < 서지정보 < 상세정보 - 특허·실용신안 정보:/Users/jonghoon.seo/Zotero/storage/QCPU3FAK/biblioa.html:text/html}
}

@patent{hong_mobile_2017,
	title = {Mobile {Terminal} and {Method} for {Controlling} the {Same}},
	abstract = {Disclosed herein are an electronic device and a method for controlling the electronic device. In an aspect, when a mirroring connection request is received from a first external device, a second external device pairing-connected with the first external device is also mirrored. Accordingly, a mirroring screen for the first external device and a mirroring screen for the second external device can be displayed together.},
	assignee = {Lg Electronics Inc},
	number = {EP3171309A1},
	author = {Hong, Soonwon and Jang, Jeonghun and Kim, Suhwang and Seo, Jonghoon and Yang, Hongbin},
	month = may,
	year = {2017}
}

@patent{han_method_2016,
	title = {{METHOD}, {DEVICE}, {COMPUTER} {READABLE} {RECORDING} {MEDIUM} {AND} {COMPUTER} {PROGRAM} {FOR} {INPUTTING} {LETTERS}},
	url = {http://goo.gl/5mRFtE},
	nationality = {ko},
	number = {10-1629737},
	author = {Han, Tack-Don and Seo, Jonghoon and Chae, Seungho and Kang, Nahyung and Kim, Dongchul},
	month = jun,
	year = {2016}
}
